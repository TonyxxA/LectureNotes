<!DOCTYPE html>
<html lang="en">

<head>
        <meta charset="UTF-8">
        <title>Sample Lecture Notes99</title>
</head>

<body>
<h2>Nerd Info</h2>
<ul>
        <li>Generated on: 08/15/2024, 12:35:50</li>
        <li>Lecture Duration: 60.0 min 13.2 sec</li>
        <li>LLM Input Tokens: 36692</li>
        <li>LLM Output Tokens: 5489</li>
        <li>Total Cost: $0.0161</li>
</ul>


<h1>Executive Summary</h1>
<p>This lecture provides a comprehensive overview of the fundamental concepts in linear algebra, starting with the basic operations on vectors and their linear combinations. The lecturer then introduces matrices as a way to represent these linear combinations, and demonstrates how to perform matrix-vector multiplication to solve systems of linear equations. A key focus is on the concept of invertible matrices, which have the property that the system of linear equations Ax = b has a unique solution for any vector b. The lecturer also discusses the notion of subspaces and bases, which are essential for understanding the structure of vector spaces. Throughout the lecture, the importance of linear algebra in various applications, particularly in computational and digital systems, is emphasized. The lecture covers a range of important topics in a clear and engaging manner, providing students with a solid foundation in this fundamental area of mathematics.</p>

<h1>Detailed Notes</h1>

<h2>1. Vectors and their Linear Combinations</h2>
<p>The lecture begins by discussing vectors and their linear combinations. The speaker states that the key operation with vectors is to take their <b>linear combinations</b>, which involves multiplying the vectors by <b>scalars</b> and then adding or subtracting them.</p>

<p>The speaker provides a specific example using three vectors, <b>u</b>, <b>v</b>, and <b>w</b>, where:</p>
$$u = \begin{bmatrix} 1 \\ -1 \\ 0 \end{bmatrix}, \quad v = \begin{bmatrix} 0 \\ 1 \\ -1 \end{bmatrix}, \quad w = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$$
<p>The speaker explains that the linear combination of these vectors can be expressed as:</p>
$$\mathbf{b} = x_1 \mathbf{u} + x_2 \mathbf{v} + x_3 \mathbf{w}$$
<p>where <b>x<sub>1</sub></b>, <b>x<sub>2</sub></b>, and <b>x<sub>3</sub></b> are the scalars.</p>

<h2>2. Matrices and Matrix-Vector Multiplication</h2>
<p>The speaker then introduces the concept of matrices and how they can be used to represent the linear combinations of vectors. The matrix <b>A</b> is formed by placing the vectors <b>u</b>, <b>v</b>, and <b>w</b> as the columns:</p>
$$\mathbf{A} = \begin{bmatrix} 1 & 0 & 0 \\ -1 & 1 & 0 \\ 0 & -1 & 1 \end{bmatrix}$$
<p>The speaker explains that multiplying the matrix <b>A</b> by the vector <b>x</b> = <b>[x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>]<sup>T</sup></b> gives the vector <b>b</b>, which is the linear combination of the columns of <b>A</b> weighted by the elements of <b>x</b>:</p>
$$\mathbf{b} = \mathbf{A}\mathbf{x}$$
<p>The speaker then walks through the step-by-step process of computing the matrix-vector multiplication, showing that the resulting vector <b>b</b> is the linear combination of the columns of <b>A</b> weighted by the elements of <b>x</b>.</p>

<h2>3. Invertible Matrices and their Inverses</h2>
<p>The speaker then introduces the concept of an <b>invertible matrix</b>, which is a matrix that has a unique inverse matrix that can transform the vector <b>b</b> back to the vector <b>x</b>:</p>
$$\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}$$
<p>The speaker explains that the inverse matrix <b>A<sup>-1</sup></b> is a <b>sum matrix</b>, as it takes the differences computed by the <b>difference matrix</b> <b>A</b> and sums them back up to recover the original vector <b>x</b>.</p>

<p>The speaker draws a connection between the inverse relationship in linear algebra and the fundamental theorem of calculus, where the derivative is the inverse of the integral.</p>

<h2>4. Subspaces and their Properties</h2>
<p>The speaker then introduces the concept of <b>subspaces</b>, which are vector spaces within a larger vector space. The speaker explains that a subspace is the set of all linear combinations of a set of vectors.</p>

<p>The speaker uses the example of the matrix <b>C</b>, which is formed by modifying the vector <b>w</b> in the matrix <b>A</b>:</p>
$$\mathbf{C} = \begin{bmatrix} 1 & 0 & -1 \\ -1 & 1 & 0 \\ 0 & -1 & 1 \end{bmatrix}$$
<p>The speaker explains that the columns of <b>C</b> are <b>dependent</b>, meaning that their linear combinations do not span the entire 3-dimensional space. Instead, they form a <b>plane</b> through the origin, which is a 2-dimensional subspace of the 3-dimensional space.</p>

<p>The speaker further discusses the properties of subspaces, including the fact that the zero vector is always a member of a subspace, and that the entire vector space itself is also considered a subspace.</p>

<h2>5. Connections between Linear Algebra and Calculus</h2>
<p>The speaker draws a connection between linear algebra and calculus, stating that the fundamental theorem of calculus is about the inverse relationship between differentiation and integration, which is analogous to the inverse relationship between the <b>difference matrix</b> and the <b>sum matrix</b> in linear algebra.</p>

<p>The speaker suggests that the course will further explore the connections between linear algebra and its applications, including the role of the <b>transpose matrix</b> and the <b>A<sup>T</sup>A</b> matrix in various contexts.</p>

<h1>Key Points</h1>
<ul>
<li><b>Vectors</b> can be combined through <b>linear combinations</b>, where vectors are multiplied by scalars and added together.</li>
<li>A <b>matrix</b> can be formed by placing vectors as its columns, and matrix-vector multiplication computes the linear combinations of the columns.</li>
<li>If a matrix <b>A</b> maps <b>x</b> to <b>b</b> through <b>Ax = b</b>, then the <b>inverse matrix A^-1</b> can map <b>b</b> back to <b>x</b> through <b>x = A^-1 b</b>.</li>
<li>The inverse matrix <b>A^-1</b> represents a "sum transform" that is the inverse of the "difference transform" represented by <b>A</b>.</li>
<li>The fundamental theorem of calculus states that integration is the inverse operation of differentiation, analogous to the inverse relationship between the difference and sum transforms in linear algebra.</li>
<li>A <b>subspace</b> is a set of vectors that is closed under linear combinations, meaning it contains all possible linear combinations of its vectors.</li>
<li>The <b>dimension</b> of a subspace is the number of linearly independent vectors that form a basis for the subspace.</li>
<li>The smallest subspaces are the zero vector and the entire vector space, while planes and lines are examples of proper subspaces in 3-dimensional space.</li>
<li>For a rectangular matrix <b>A</b>, the matrix <b>A^T A</b> is always square and symmetric, and it plays an important role in many applications.</li>
</ul>

<h1>Action Items</h1>
<ul>
<li>Review the examples of matrices <b>A</b> and <b>C</b> discussed in the lecture and understand the differences in their properties and solutions to the linear systems <b>Ax = b</b> and <b>Cx = b</b>.</li>
<li>Familiarize yourself with the concept of subspaces and their properties, including the idea of a basis and the dimension of a subspace.</li>
<li>Explore the connections between linear algebra and calculus, particularly the relationship between differentiation and integration as inverse operations.</li>
<li>Review the linear algebra resources mentioned, such as the 18.06 website, to further solidify your understanding of the key concepts covered in the lecture.</li>
</ul>

<h1>AI Lecture Study Guide</h1>

<h2>Introduction to Linear Algebra</h2>
<p>The lecture begins by emphasizing the importance of linear algebra, noting that it is widely used in the real world for tasks like solving differential equations and working with matrices. The lecturer then provides an overview of the key concepts in linear algebra, starting with vectors and their linear combinations.</p>

<h3>Vectors and Linear Combinations</h3>
<p>The lecturer introduces the concept of <b>vectors</b> and <b>linear combinations</b> of vectors. A linear combination of vectors $\mathbf{u}$, $\mathbf{v}$, and $\mathbf{w}$ is given by the expression:</p>

$$\mathbf{x} = x_1\mathbf{u} + x_2\mathbf{v} + x_3\mathbf{w}$$

<p>where $x_1$, $x_2$, and $x_3$ are <b>scalars</b> (real numbers) that multiply the respective vectors.</p>

<h3>Matrices and Matrix-Vector Multiplication</h3>
<p>The lecturer then introduces the concept of <b>matrices</b>, where the vectors $\mathbf{u}$, $\mathbf{v}$, and $\mathbf{w}$ are placed as the columns of a matrix $\mathbf{A}$:</p>

$$\mathbf{A} = \begin{bmatrix} 
1 & 0 & 0 \\
-1 & 1 & -1 \\
0 & -1 & 1
\end{bmatrix}$$

<p>The matrix-vector multiplication $\mathbf{A}\mathbf{x}$ then computes the linear combination of the columns of $\mathbf{A}$ using the scalars $x_1$, $x_2$, and $x_3$ as the coefficients.</p>

<h3>Solving the Matrix Equation $\mathbf{A}\mathbf{x} = \mathbf{b}$</h3>
<p>The lecturer then discusses solving the matrix equation $\mathbf{A}\mathbf{x} = \mathbf{b}$, where $\mathbf{b}$ is a given vector. For the matrix $\mathbf{A}$ shown earlier, the solution can be found by:</p>

$$\mathbf{x} = \begin{bmatrix}
b_1 \\
b_2 - b_1 \\
b_3 - b_2
\end{bmatrix}$$

<p>This solution can be expressed as a matrix-vector multiplication $\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}$, where $\mathbf{A}^{-1}$ is the <b>inverse matrix</b> of $\mathbf{A}$.</p>

<h3>Invertible Matrices and Subspaces</h3>
<p>The lecturer then discusses the concept of <b>invertible matrices</b>, which are matrices that have an inverse. Invertible matrices correspond to linear transformations that are bijective (one-to-one and onto).</p>

<p>The lecturer also introduces the concept of <b>subspaces</b>, which are vector spaces contained within a larger vector space. The set of all linear combinations of the columns of a matrix $\mathbf{A}$ forms a subspace, and the dimension of this subspace is equal to the number of linearly independent columns in $\mathbf{A}$.</p>

<h2>Dependent and Independent Vectors</h2>
<p>The lecture then explores the differences between <b>dependent</b> and <b>independent</b> vectors. Dependent vectors are those that can be expressed as a linear combination of other vectors, while independent vectors cannot be expressed in this way.</p>

<p>The lecturer provides two examples to illustrate the concepts of dependent and independent vectors:</p>

<h3>Example 1: Independent Vectors</h3>
<p>In the first example, the lecturer considers the vectors:</p>

$$\mathbf{u} = \begin{bmatrix} 1 \\ -1 \\ 0 \end{bmatrix}, \quad \mathbf{v} = \begin{bmatrix} 0 \\ 1 \\ -1 \end{bmatrix}, \quad \mathbf{w} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$$

<p>These vectors are linearly independent, meaning that no vector can be expressed as a linear combination of the others. The set of all linear combinations of these vectors spans the entire 3-dimensional space $\mathbb{R}^3$.</p>

<h3>Example 2: Dependent Vectors</h3>
<p>In the second example, the lecturer considers the vectors:</p>

$$\mathbf{u} = \begin{bmatrix} 1 \\ -1 \\ 0 \end{bmatrix}, \quad \mathbf{v} = \begin{bmatrix} 0 \\ 1 \\ -1 \end{bmatrix}, \quad \mathbf{w}^* = \begin{bmatrix} -1 \\ 1 \\ 1 \end{bmatrix}$$

<p>In this case, the vector $\mathbf{w}^*$ can be expressed as a linear combination of $\mathbf{u}$ and $\mathbf{v}$, so the vectors are linearly dependent. The set of all linear combinations of these vectors forms a 2-dimensional subspace within $\mathbb{R}^3$.</p>

<h2>Practical Applications and Further Study</h2>
<p>The lecture highlights the importance of linear algebra in various practical applications, such as solving differential equations, working with digital systems, and analyzing networks. The lecturer also suggests that students explore the connections between linear algebra and calculus, as the fundamental theorem of calculus can be seen as the inverse relationship between differentiation and integration, similar to the inverse relationship between the difference and sum matrices discussed in the lecture.</p>

<h2>Practice Problems</h2>
<p>1. Find the matrix $\mathbf{A}^{-1}$ that is the inverse of the matrix $\mathbf{A}$ given in the lecture:</p>

$$\mathbf{A} = \begin{bmatrix} 
1 & 0 & 0 \\
-1 & 1 & -1 \\
0 & -1 & 1
\end{bmatrix}$$

<p>2. Determine whether the following set of vectors is linearly independent or linearly dependent:</p>

$$\mathbf{u} = \begin{bmatrix} 2 \\ 1 \\ 0 \end{bmatrix}, \quad \mathbf{v} = \begin{bmatrix} 1 \\ -1 \\ 2 \end{bmatrix}, \quad \mathbf{w} = \begin{bmatrix} 3 \\ 0 \\ 1 \end{bmatrix}$$

<p>3. Solve the matrix equation $\mathbf{C}\mathbf{x} = \mathbf{b}$, where:</p>

$$\mathbf{C} = \begin{bmatrix}
1 & -1 & -1 \\
0 & 1 & -1 \\
0 & 0 & 1
\end{bmatrix}, \quad \mathbf{b} = \begin{bmatrix}
2 \\ 1 \\ 3
\end{bmatrix}$$

<h2>Practice Problems Solutions</h2>
<p>1. To find the inverse matrix $\mathbf{A}^{-1}$, we can use the formula:</p>

$$\mathbf{A}^{-1} = \begin{bmatrix}
1 & 0 & 0 \\
1 & 1 & 1 \\
0 & 1 & 1
\end{bmatrix}$$

<p>2. To determine if the set of vectors is linearly independent, we need to check if any vector can be expressed as a linear combination of the other two. In this case, we can see that $\mathbf{w} = 3\mathbf{u} - \mathbf{v}$, so the vectors are linearly dependent.</p>

<p>3. To solve the matrix equation $\mathbf{C}\mathbf{x} = \mathbf{b}$, we can use the same approach as in the lecture. First, we can rewrite the system of equations as:</p>

$$x_1 - x_2 - x_3 = 2$$
$$x_2 - x_3 = 1$$
$$x_3 = 3$$

<p>Solving this system, we get:</p>

$$\mathbf{x} = \begin{bmatrix}
6 \\ 3 \\ 3
\end{bmatrix}$$

<h2>Further Study</h2>
<p>For students interested in further exploring the concepts covered in this lecture, I recommend the following resources:</p>

<ul>
  <li>Gilbert Strang's "Introduction to Linear Algebra" textbook, which provides a comprehensive and accessible introduction to the subject.</li>
  <li>The MIT OpenCourseWare course "18.06 Linear Algebra", which includes additional examples, practice problems, and video lectures on linear algebra.</li>
  <li>Khan Academy's linear algebra playlist, which covers the fundamental concepts in a step-by-step manner with interactive exercises.</li>
  <li>The "Linear Algebra" course on Coursera, which offers a more advanced and application-oriented perspective on the subject.</li>
</ul>

<p>Additionally, students may want to explore the connections between linear algebra and other fields, such as calculus, differential equations, and numerical analysis, to gain a deeper understanding of the subject's broader applications.</p>
<h2>Appendix</h2>
<h3>Transcript</h3><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. OK. So this is the one and only review, you could say, of linear algebra. My website, I just think linear algebra is very important. You may have got that idea. And my website even has a little essay called Too Much Calculus. Because I think it's crazy for all the US universities to do this pretty much. You get semester after semester, differential calculus, integral calculus, ultimately differential equations. You run out of steam before the good stuff or you run out of time. And anybody who computes, who's living in the real world, is using linear algebra. You're taking a differential equation, you're taking your model, making it discrete, and computing with matrices. I mean, that's the world's digital now, not analog. So I hope it's OK to start the course with linear algebra. And so, but many engineering curricula don't fully recognize that. And so if you haven't had a official course, linear algebra, stay with 18085. This is a good way to learn it. You sort of learning what's important. So my review would be, and then this is, so future Wednesdays will be in our regular room for homework, review, questions of all kinds. And today, questions too. Shall I just fire away for the first half of the time to give you a sense of how I see the subject or least within that limited time? And then questions are totally welcome. Always welcome, actually. So I'll just start right up. So essentially, linear algebra progresses, starting with vectors to matrices. And then finally to subspaces. So that's like the abstraction, you could say abstraction, but it's not difficult, that you want to see. Until you see the idea of a subspace, you haven't really got linear algebra. OK, so I'll start at the beginning. What do you do with vectors? Answer, you take their linear combinations. That's what you can do with a vector. You can multiply it by a number, and you can add or subtract. So that's the key operation. Suppose I have vectors u, v, and w. Let me take three of them. And so I can take their combinations. So some combination will be, say, some number times u plus some number times v plus some number times w. So these numbers are called scalars. So these would be called scalars. And the whole thing is a linear combination. Let me abbreviate those words, linear combination. And you get some answer, say b. OK. But let's pin it down, make this whole discussion specific. Yeah, I started it a little early, I think. So let me, I'm going to take three vectors, u, v, and w. And take their combinations. So my u is going to be, they're carefully chosen. My u is going to be 1 minus 1, 0. And I'll take vectors in three dimensions. So that means their combinations will be in three dimensions. R3, three-dimensional space. So that will be u, and then v. So let's take 0, I think, 1 and minus 1. OK. Suppose I stopped there and took their linear combinations. It's very helpful to see a picture in three-dimensional space. I mean, the great thing about linear algebra, it moves into n-dimensional space, 10-dimensional, 100-dimensional, where we can't visualize. But yet, our instinct is right if we just follow. So what's your instinct? If I took those two vectors and notice they're not on the same line. One isn't a multiple of the other. They go in different directions. If I took their combinations, x1 times u plus x2 times v. Oh, now this is, oh, let me push. This is a serious question. If I took all their combinations. So let me try to draw a little bit. OK. I'm in three-dimensional space. And u goes somewhere, maybe there. And v goes somewhere, maybe here. Now suppose I take all the combinations. So I could multiply that first guy by any number. That would fill the line. I could multiply that second guy v. So this was u, and this was v. I could multiply that by any number x2. That would fill its line. Each of those lines I would later call a one-dimensional subspace, just a line. But now what happens if I take all combinations of the two? What do you think? You get a plane. Get a plane. If I take anything on this line and anything on this line and add them up, you can see that I'm not going to fill 3D. But I'm going to fill a plane. And that maybe takes a little thinking. It just, then it becomes sort of, yeah, you see that that's what it has to be. OK. Now I'm going to have a third vector. OK. My third vector will be 001. OK. So that 001 is 0 in the x, 0 in the y, and 1 in the z direction. So there's w. OK. Now I want to take their combinations. So let me do that very specifically. How do I take combinations? This is important. Seems it's very simple, but important. I like to think of taking the combinations of some vectors. I'm always putting vectors into the columns of a matrix. So now I'm going to move to step two, matrix. I'm going to move to step two, and maybe I'll put it, well, not better, put it here. OK. Step two is the matrix has those vectors in its columns. So in this case, it's 3 by 3. OK. That's my matrix that I'm going to call it A. And now I'm going to, how do I take combinations of vectors? I, well, I should have maybe done it in detail here, but let me, I'll just do it with the matrix here. Watch this now. I'm going to, if I multiply A by the vector of x's, what that does? So this is now A times x. So very important of matrix times the vector. What does it do? The output is just what I want. This is the output. It takes x1 times the first column plus x2 times the second plus x3 times the third. That's the way matrix multiplication works by columns. And you don't always see that. Because what do you see? You know how to probably know how to multiply that matrix by that vector. Let me ask you to do it. OK. What do you get? So everybody does it a component at a time. So what's the first component of the answer? x1. Yeah. How do you get that? It's a row times the vector. And when I say times, I really mean that dot product. This time plus this plus this is x1. And what about the second row? Minus x1 plus x2, or I'll just say x2 minus x1. And the third guy, the third component would be x3 minus x2. OK. So right away, I'm going to call this matrix a a difference matrix. It always helps to give names to things. So this a is a difference matrix. Because it takes differences of the x's. And I would even say a first difference matrix. Because it's just straightforward difference. And we'll see second differences in class Friday. OK. So that's what a does. But remember my first point was that when a matrix multiplies a vector, the result is a combination of the columns. And that's not always because, see, I'm looking at the picture not just by numbers. With numbers, I'm just doing this stuff. But now I'm stepping back a little bit and saying, I'm combining, it's this vector times x1. That vector times x1 plus this vector times x2 plus that one times x3 added together gives me this. Same, nothing complicated here. It's just look at it by vectors also. OK. Now suppose, so it's a little interesting. Already. Here we multiply these vectors by numbers. x1, x2, x3. That was our thinking here. Now our thinking here is a little we've switched slightly. Now I'm multiplying the matrix times the numbers in x. Just a slight switch. Multiply the matrix times the number. And I get some answer B, which is this. This is B. OK. And of course, I can do a specific example, like suppose I take the squares to be in x. So suppose I take A times the first three squares, 1, 4, 9. What answer would I get? Just to keep it clear that we're very specific here. So what would be the output B? I think of this as the input, the 1, 4, 9, the x's. Now the machine is a multiply by A. And there, here's the output. And what would be the output? What numbers am I going to get there? Yeah. 1, 3, something? 1, 3, 5. Which is actually a little neat. That you find the differences of the squares are the odd numbers. That appealed to me in school somehow. And that was already a bad sign, right? This dumb kid notices it. That you take the differences of square numbers, right? Whatever. OK. So now is a big step. This was the forward direction, right? The input, and there is the output. But now the real reality, that's easy and important. But the more deep problem is, what if I give you B and ask for x? So again, we're switching the direction here. OK. We're solving an equation now, or three equations, and three unknowns, Ax equal B. So if I give you this B, can you get x? OK. How do you solve three equations? So this is just we're looking backwards. Now, that won't be too hard for this matrix, for this particular matrix that I chose, because it's triangular, we'll be able to go backwards. So let me do that. Let me take B to B. It's a vector. It's got three components. And now I'm going to go backwards to find x, or we will. OK. So do you see the three equations? Here they are. x1 is B1. This is B2. That difference is B3. Those are my three equations. Three unknown, x is three known right hand sides. Or I think of it as A times x as a matrix times x, giving a vector B. What's the answer? OK. As I said, we're going to be able to do this. We're going to be able to solve this system easily, because it's already triangular. And it's actually lower triangular. So that means we'll start from the top. So the answer is the solution will be what? Let's make room for it. x1, x2, and x3. I want to find. And what's the answer? What can we just go from the top to bottom now? What's x1? B1. Great. What's x2? So x2 minus x1. These are my equations. So what's x2 minus x1? Well, it's B2. So what is x2? B1 plus B2, right? And what's x3? What do we need there for x3? So I'm looking at the third equation. That'll determine x3. When I see it this way, I see those ones. And I see it multiplying x3. And what do I get? Yeah. So x3 minus this guy is B3. So I have to add in another B3, right? I'm doing sort of substitution down as I go. Once I learned that x1 was B1, I used it there to find x2. And now I'll use x2 to find x3. And what do I get again? x3 is I put the x2 over there. I think you've got it B1 plus B2 plus B3. OK. So that's a solution. Not difficult because the matrix was triangular. But let's think about that solution. That solution is a matrix times B. When you look at that, so this is like a good early step in linear algebra. When I look at that, I see a matrix multiplying B. I mean, you take that step up to seeing a matrix. And you can just read it off. So let me say, what's the matrix there? That's multiplying B to give that answer. From remember, the columns of this matrix, well, I don't know how you want to read it off. But one way is to think the columns of that matrix are multiplying B1, B2, and B3 to give this. So what's the first column of the matrix? It's whatever I'm reading off. The coefficients really of B1 here, 1, 1, 1. And what's the second column of the matrix? 0, 1, 1. Good. 0, B2 is 1, 1. And the third is 0, 0, 1. Good. OK. Now, so lots of things to comment here. This is, let me write up again here. This is x. That was the answer. It's a matrix times B. And what's the name of that matrix? It's the inverse matrix. If Ax gives B, the inverse matrix does it the other way round. x is a inverse B. Let me just put that over here. If Ax is B, then x should be a inverse B. You could, so we had inverse, I wrote down inverse this morning, but without saying the point. But so you see how that comes. I mean, if I want to go formally, I multiply both sides by A inverse. If there is an A inverse, that's a critical thing, as we saw. Is the matrix invertible? The answer here is yes. There is an inverse. And what does that really mean? The inverse is the thing that takes us from B back to x. Think of A as multiplying by A as kind of a mapping, mathematicians use the word, or transform. Transform would be good. Transform from x to B. And this is the inverse transform. So it doesn't happen to be the discrete Fourier transform or a wavelet transform. Well, actually, we could give it a name. This is kind of a difference transform, right? That's what A did. Took differences. So what does A inverse do? It takes sums. It takes sums. That's why you see 1, 1, 1, 1, 1 along the rows, because it's just adding, and you see it here, and fully displayed. It's a sum matrix. I might as well call it S for sum. So that matrix, that sum matrix, is the inverse of the difference matrix. Yeah. OK. And maybe I just sent a hit on calculus earlier. You could say the calculus is all about one thing, and it's inverse. The derivative is A, and what's S? In calculus. The integral. The whole subject is about one operation. Now, admittedly, it's not a matrix. It operates on functions instead of just little vectors. But that's the main point. The fundamental theorem of calculus is telling us that integrations, the inverse of difference, H. OK. So this is good, and I could put in, if I put in B equal 1, 3, 5, for example, just to put in some numbers. If I put in B equal 1, 3, 5, what would the x that comes out B? 1, 4, 9. Because it takes us back. Here previously, we took differences of 1, 4, 9, got 1, 3, 5. Now, if we take sums of 1, 3, 5, we get 1, 4, 9. OK. Now, we have a system of linear equation. Now, I want to step back and see what was good about this matrix, somehow it has an inverse. A, x equal B has a solution, in other words. And it has only one solution, right? Because we worked it out. We had no choice. That was it. So there's just one solution. There's always one, and only one solution. It's like a perfect transform from the x's to the b's and back again. So that's what an invertible matrix is. It's a perfect map from one set of x's to the b's, and you can get back again. OK. Right. Yeah, questions always. OK. Now, I think I'm ready for another example. There are only two examples. And actually, this example, these two examples are on the 1806 web page. If you want to, some people asked after class, OK, how to get sort of a review of linear algebra? Well, the 1806 website would be definitely a possibility. So that's, well, I'll put down the open courseware website, m-m-i-t-e-d-u, and then you would look at the linear algebra course, or the math one. What is it? Web.math.edu, is that it? No, maybe that's an MIT. Not, so is it math? I can't live withoutedu at the end, right? Yeah. Is it justedu? Whatever. And then that, yeah. OK. So that website has, well, all the old exams you could ever want, if you wanted any. And it has this example, and you click on starting with two matrices. And this is one of them. OK, ready for the other. So here comes the second matrix, second example that you can contrast. All right, second example is going to have the same u. Let me put our matrix, I'm going to call it, what am I going to call it? Maybe c. OK, so it will have the same u and the same v. But I'm going to change w. And that's going to make all the difference. My w, I'm going to make that into w. OK. So now I have three vectors. I can take their combinations. I can look at the equation cx equal b. I can try to solve it. All the normal stuff. With those combinations of those three vectors. OK, so, and we'll see a difference. OK, so now what happens if I do, can I even like do just a little a race to deal with c now? How does c differ? If I change this multiplication from a to c to this new matrix, then what we've done is to put in a minus 1. That's the only change we made. And what's the change in cx? This, I've changed the first rows. I'm going to change the first row of the answer to what? x1 minus x3. You could say again, as I said this morning, you've sort of changed the boundary condition maybe. You've made this difference equation somehow circular. That's why you're using that letter c. Yeah? Is it different? Oh, yes. I didn't get it right here. Thank you. Thank you very much. Absolutely. I mean, that would have been another matrix that we could think about, but it wouldn't have made the point I wanted. So, I'm, thanks, that's absolutely great. So, let me, yeah, so now it's correct here. And this is correct. And I can look at equations. But can I solve them? Can I solve them? And you're guessing already, no, we can't do it. Because, so now, let me maybe go to a board, work below, because I hate to erase that was so great. That being able to solve it in a nice clear solution and some matrix coming in. But now, how about this one? OK. So, one comment I should have made here. Suppose the bees were zero. Suppose I was looking at originally at a times x equal all zeros. What's x? If all the bees were zero, and this was the one that dealt with the matrix a, if all the bees are zero, then the x's are zero. The only way to get zero right-hand sides, bees, was to have zero x's. It didn't, if you wanted to get zero out, you had to put zero in. Well, you can always put zero and get zero out. But here, you can put other vectors in and get zero out. So, I want to say there's a solution with zeros out coming out of C. But some non-zeroes are going in. And of course, we know from this morning that that's a signal that it's a different sort of matrix. There won't be an inverse. We've got questions. Now, what is, what are the, tell me all the solutions? All the solutions. So, actually, not just one, well, you could tell me one. Tell me one for it. One, one, one. Now, tell me all. C, C, C, yeah. That whole line through one, one, one. And that would be normal. So, this is a line of solutions. A line of solutions, I think, of one, one, one is in some solution space. And then all multiples, that whole line. Later, I would say, it's a subspace. When I say what that word subspace means, it's just this linear algebra has done its job beyond just one, one, one, okay. So, and again, it's just, it's a fact of, if we only know the differences, yeah, you can see different ways that this has got problems. So, that's C times x. Now, we can, one way to see a problem is to say, we can get the answer of all zeros by putting constants. All that's saying is, in words, the differences of a constant vector are all zeros, right? That's all that happened. Another way to see a problem, if I had, if I had this system of equations, how would you see that there's a problem? And how would you see that there is sometimes an answer and even decide when? I don't know if you can take a quick look. If I had three equations, x1 minus x3 is b1. This equals b2. This equals b3. Why, yeah. Do you see something that I can do to the left sides? That's important somehow. Suppose I add those left hand sides. What do I get? So, and I'm allowed to do that, right? If I've got three equations, I'm allowed to add them. And I would get zero, if I add, I get zero equals, b, adding, I have to add the right sides, of course. b1 plus b2 plus b3. That's, I hesitate to say a fourth equation because it's not independent of those three, but it's a consequence of those three. So, actually, this is telling me when I could get an answer and when I couldn't. If I get zero on the left side, I have to have zero on the right side, or I'm lost. So, I could actually solve this when b1 plus b2 plus b3 is zero. That would be, that would, now, so I've taken a step there. I've said that, okay, we're in trouble often. But in case the right side adds up to zero, then we're not. And if you'll allow me to jump to a mechanical meaning of this, if these were springs or something, masses, and these were forces on them. So, I'm solving for displacements of masses that we'll see very soon, and these are forces. What I'm saying is, what that equation is saying is, because they're sort of cyclical, it's somehow saying that if the forces add up to zero, if the resultant force is zero, then you're okay. The springs and masses don't take off, or start spinning, or whatever. So, there's a physical meaning for that condition that it's okay, provided, if the b's add up to zero. But of course, if the b's don't add up to zero, we're lost. Right, yeah, yeah. Okay, so ax equal b could be solved. Cx equal b could be solved sometimes, but not always. And so you see that we're seeing the, the difficulty with C is showing up several ways. It's showing up in a C times a vector x giving zero. That's bad news, because no C inverse can bring you back. I mean, it's like you can't come back from zero. Once you've got to zero, C inverse could never bring you back to x. Right? A took x into b up there, and then A inverse brought back x. But here, there's no way to bring back that x, because I can't multiply zero by anything and get back to x. So that's why I see it's got troubles here. Here I see it's got troubles because if I add the left sides, I get zero. And therefore, the right sides must add to zero. So you've got trouble several ways. Let's see another way. Let's see geometrically why we're in trouble. Okay, so let me draw a picture to go with that picture. So there's three dimensional space. I didn't change u. I didn't change v, but I changed w to minus one. What does that mean? Minus one sort of going this way, maybe. Zero one is the z direction. Somehow I changed it to there. So this is w star, maybe a different w. This is the w that gave me problems. What's the problem for the, how does the picture show the problem? Oh, I'm not sure. I don't, my, my, what's the problem with those three vectors, those three columns of c? And why are they, yeah? They're in the same plane. W star gave us nothing new. We had the combinations of u and v made a plane and w star happened to fall in that plane. So this is a plane here, somehow, and, and, and, go through the origin, of course. What is that plane? This is all combinations, all combinations of u, v, and the third guy, w star. So this is the, this is the, this is the, this is the, this is the, u, v, and the third guy, w star. Right. Is a plane and I drew a triangle, but of course, I should draw the plane goes out to infinity. But the point is there are lots of b's, lots of right hand sides, not on that plane. Okay. Now what, what would the, if I drew all combinations of u, v, w, the original w, what have I got? So let me bring that picture back for a moment. If I took all combinations of those, does w lie in the plane of u and v? No, right. I would call it independent. These three vectors are independent. These three, u, v, and w star, I would call dependent. Because I did not, the third guy was a combination of the first two. Okay. So yeah, tell me what do I get now? So now you're really up to 3d. What do you get if you take all combinations of u, v, and w? Say it again. The whole space, all combinations, if taking all combinations of u, v, w, give you the whole space. Why is that? Well, we just showed when it was A, we showed how did, we showed that we could get every B. We wanted the combination that gave B, and we found it. So at the beginning when we were working with u, v, w, we found, if this was the shorthand here, we, this said find a combination to give B, and this says that combination will work. And we wrote out what X was. Now what's the difference? Okay, here. So that, those were dependent. Those were, sorry, those were independent. I would even call those three vectors a basis for three dimensional space. That word basis is a big deal. So a basis for five dimensional space is five vectors that are independent. That's one way to say it. The second way to say it would be their combinations give the whole five dimensional space. A third way to say it, see if you can finish this sentence. This is for the independent, the good guys. If I put those five vectors into a five, five, five matrix, that matrix will be, invertible, that matrix will be invertible. So an invertible matrix is one with a basis sitting in its columns. It's a, it's a transform that you, that has an inverse transform. This matrix is not invertible. Those three vectors are not a basis. Their combinations are only in a plane. By the way, a plane is a subspace. So a plane would be a typical subspace. You know, it's like it filled it out. You took all the combinations, you did your job. But in that case, the whole space would be a count as a subspace too. That's the way you get subspaces by taking all combinations. Okay, now I'm even gonna push you one more step and then this example is complete. Can you tell me this is, what vectors do you get? All combinations of u, v, w. Let me try to write something. This gives only a plane. Because we've got two independent vectors but not the third. Okay. I don't know if I should even ask. Did we know an equation for that plane? Well, I think we do if we think about it correctly. All vectors, all combinations of u, v, w star is the same as saying, all vectors, c times x, right? Very, yeah. Do you agree that those two are exactly the same thing? This is the key because we're moving up to vectors, combinations and now come subspaces. If I take all combinations of u, v, w star, I say that that's the same as all vectors, c times x, why is that? It's what I said in the very first sentence at four o'clock. The combinations of u, v, w star, how do I produce them? I create the matrix with those columns. I multiply them by x's and I get all the combinations. And this is just c times x. So what I've said there is just another way of saying, how does matrix multiplication work? Put the guys in its columns and multiply by a vector. Okay. So that's, so we're getting all vectors c times x. And now I was going to stretch it that little bit further. Can we see what can we describe what vectors we get? So that's my question. What b's, what b's? So this is b equal b1, b2, b3. Do we get? We don't get them all. That's, right? We don't get them all. That's the trouble with c. We only get a plane of them. And now can you tell me which b's we do get when we look at this, at all combinations of these three dependent vectors? Well, we've done a lot today. Let me just tell you the answer because it's here. The b's have to add to zero. That's the equation that the b's have to satisfy if, because when we wrote out cx, we noticed that we always got the components always added to zero. So the, which b's do we get? We get the ones where the components add to zero. In other words, there's, that's the equation of the plane, you could say. Yeah, actually, that's a good way to look at it. All these vectors are on the plane. Do the components of u add to zero? Look at u. Yes. Do the components of v add to zero? Yes, add them up. Do the components of w star? Now that you fix it correctly for me. Do they add to zero? Yes, so all the combinations will add to zero. That's the plane. That's the plane. You see, there's so many different ways to see. And none of this is difficult, but it's coming fast because we're seeing the same thing in different languages. We're seeing it geometrically in a picture of a plane. We're seeing it by the combination of vectors. We're seeing it as a multiplication by a matrix. And we saw it sort of here by operation, operating and simplifying and getting the key fact out of the equations. Well, okay. I wanted to give you this example because the two examples, because they bring out so many of the key ideas. The key idea of a subspace, shall I just say a little? What that word means? A subspace. What's a subspace? Well, what's a vector space, first of all? A vector space is a bunch of vectors. And the rule is you have to be able to take their combinations. That's what linear algebra does, takes combination. So a vector space is one where you take all combinations. So yeah, so it finally took like just this triangle. That would not be a subspace because one combination would be two u and it would be out of the triangle. So a subspace, just think of it as a plane. But then, of course, it could be in higher dimensions. It could be a seven-dimensional subspace inside 15-dimensional space. And I don't know if you're good at visualizing that. But I'm not. But it's never mind. You've got seven vectors. You think, OK, there are combinations to give a seven-dimensional subspace. Each vector has 15 components. No problem. I mean, no problem for MATLAB. Certainly, it's got a matrix with 105 entries. It deals with that instantly. OK. So a subspace is like a vector space inside a bigger one. That's why the prefix sub is there. And mathematics always counts the biggest possibility, too, which would be the whole space. And what's the smallest? So what's the smallest subspace of r3? So I have three-dimensional space. You could tell me all the subspaces of r3. So there is one, a plane. Yeah, tell me all the subspaces of r3. And then you'll have that word kind of down. A line. So planes and lines, you could say, the real, the proper subspaces, the best, the right ones. But there are a couple more possibilities, which are a point, which point? The origin. The only of the origin. Because if you tried to say that point was a subspace, no way. Why not? Because I wouldn't be able to multiply that vector by 5, and I would be away from the point. But the 0 subspace, the really small subspace, it just has the 0 vector. It's got one vector in it. Not empty. It's got that one point, but that's all. OK, so planes, lines, the origin, and then the other possibility for a subspace is the whole space. So the dimensions could be 3 for the whole space, 2 for a plane, 1 for a line, 0 for a point. It's just, yeah, it's just kicks together. OK, how are we for a time? Maybe it went more than the half. Now is a chance to just ask me if you want to, like anything about the course. Is it all linear algebra? No. But I think I can't do anything more helpful to you than to, for you to see. Begin to see, when you look at a matrix, begin to see what is it doing? What is it about? And of course matrices can be rectangular. So I'll give you a hint about what's coming in the course itself, is we'll have rectangular matrices A. OK, they're not invertible. They're taking seven dimensional space to three dimensional space or something. No, that's the candidate for that. But what comes up every time, I sort of get the idea finally. Every time I see a rectangular matrix, maybe 7 by 3, 7, that would be 7 rows 3 column. Then what comes up with a rectangular matrix A is sooner later, A transpose sticks it's nosy. So and multiplies that A. So the matrix that I, and we could do it for the origin, for our A here. Actually, if I did it for that original matrix A, I would get something you'd recognize. So A, it's what I want to say is that the course focuses on A transpose A. And I'll just say now that that matrix always comes out square, because this would be 3 times 7 times 7 times 3. So this would be 3 by 3. And it always comes out symmetric. That's the nice thing. And even more, we'll see more. So that's like a hint of where, watch for A transpose A in what's coming. And watch for it in applications of all kinds. I mean, in networks, an A will be associated with curcups of voltage law and A transpose with curcups current law. They just, you know, they just teamed up together. We'll see more. All right, now let me give you a chance to ask any question. Whatever. Homework. So did I mention homework? You may have said that's a crazy homework to say three problems in 1.1. I've never done this before. So essentially, you can get away with anything this week. And indefinitely, actually, MIT. How many are, is this the first day of MIT classes? Oh, wow. OK. Well, welcome to MIT. And I hope you like it. It's very, it's not so high pressure or whatever you is the associated with MIT. It's kind of tolerant. You ask at least if my advisees ask for something, I will say yes, it's easier that way. And much better. I just, and let me just, I'll just again, and I'll say it often in private. So this is like a grown up course. I'm figuring you're here to learn. So it's not my job to force it. My job is to help it. And I hope this is some help.</p>
</body>
</html>